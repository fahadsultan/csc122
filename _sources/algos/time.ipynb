{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Complexity\n",
    "\n",
    "Time Complexity of an algorithm is the amount of 'time' it takes to run in terms of the size of the input, that is independent of machine and programming language. \n",
    "\n",
    "In essence, it gives us a way to compare the efficiency and scalability of algorithms.\n",
    "\n",
    "We can measure this by counting the number of operations that are performed under the assumptions of the RAM model of computation.\n",
    "\n",
    "## Counting Operations\n",
    "\n",
    "Consider the following function that takes in a list of numbers and returns the sum of the numbers.\n",
    "\n",
    "```python\n",
    "def sum_list(data):\n",
    "    total = 0\n",
    "    for num in data:\n",
    "        total += num\n",
    "    return total\n",
    "```\n",
    "\n",
    "We can count the number of operations that are performed in this function to get a sense of how long it will take to run. Under the assumptions of RAM model of computation, we assume that each operation that is not a function call or a loop takes 1 unit of time. Using this assumption, we can count the number of operations in the function above as follows:\n",
    "\n",
    "```python\n",
    "def sum_list(data):\n",
    "    total = 0          # 1 operation: assigment \n",
    "    for num in data:   # n operation where n = len(data)\n",
    "        total += num   # 2 operations: assignment + addition\n",
    "    return total       # 1 operation: return\n",
    "```\n",
    "\n",
    "In this case, 2 operations that are performed for each element in the list, so the total number of operations is 2n + 2. \n",
    "\n",
    "As a general rule, the number of operations in a function is the number of operations in a block of code is: \n",
    "\n",
    "$$\\text{Count(Code)} =  \\text{Operations outside loop(s)} + (\\text{Operations inside loop} \\times \\text{Iterations of the loop})$$\n",
    "\n",
    "Please note that this is a rough estimate of the number of operations. The heuristic above assumes a single loop. \n",
    "\n",
    "In case of multiple loops, we can use the following heuristic for counting the number of operations, for a given pair of loops: \n",
    "\n",
    "$$\\text{Total Operations for 2 loops} = \n",
    "\\begin{cases}\n",
    "    \\text{Count(Loop}_1) \\times \\text{Count(Loop}_2) & \\text{if Loop}_1 \\text{~and Loop}_2~\\text{are nested} \\\\\n",
    "    \\text{Count(Loop}_1) + \\text{Count(Loop}_2), & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "---\n",
    "\n",
    "Consider the block of code that follows and answer the Search questions below.\n",
    "\n",
    "**Question:** Given **two inputs**: 1. list of n numbers `data` and 2. a number called `query`, return **output** last location of query in list, if it exists, otherwise return -1\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def search(data, query):         # 1\n",
    "   for i in range(len(data)):    # 2\n",
    "      if data[i] == query:       # 3\n",
    "         return i                # 4\n",
    "   return -1                     # 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best, Average and Worst Case\n",
    "\n",
    "Using the RAM model of computation, we can count how many steps our algorithm takes on any given input instance by executing it. \n",
    "\n",
    "However, to understand **how good or bad** an algorithm is in general, we must know how it works **over ALL instances** (inputs).\n",
    "\n",
    "To understand the notions of the best, worst, and average-case complexity, think about running an algorithm over all possible instances of data that can be fed to it. For the problem of searching, the set of possible input instances consists of all possible integers as `query` and all possible lists of $n$ elements as `data`.\n",
    "\n",
    "We can represent each input instance as a point on a graph where the x-axis represents the size of the input problem (size/length of data), and the y-axis denotes the number of steps taken by the algorithm in this instance.\n",
    "\n",
    "```{figure} https://miro.medium.com/v2/resize:fit:4800/format:webp/1*1WlWN-d_vqf-_o48bWMjNA.png\n",
    "---\n",
    "height: 300px\n",
    "name: fig-2-1\n",
    "---\n",
    "The best, worst, and average-case complexities of an algorithm.\n",
    "```\n",
    "\n",
    "These points naturally align themselves into columns, because only integers represent possible input size (e.g., it makes no sense to sort 10.57 items). We can define three interesting functions over the plot of these points:\n",
    "\n",
    "* The **Worst-case complexity** of the algorithm is the function defined by the **maximum number of steps** taken in any instance of size _n_. This represents the curve passing through the highest point in each column. This is the blue curve in the figure above. \n",
    "\n",
    "    <br/>\n",
    "\n",
    "    In case of the search algorithm, the worst case is when the `query` is either not in `data` or is the last element in `data`, and the algorithm has to go through all the elements in the list to determine that.\n",
    "\n",
    "    The worst-case complexity proves to be **most useful** of these three measures **in practice**. The reasons for this is that the worst-case complexity is the highest stakes scenario, and therefore the most important to understand. A lack of understanding and subsequently preparedness for the worst-case scenario can often lead to high cost catastrophic consequences such as downtime, data loss, and even loss of life.\n",
    "\n",
    "    <br/>\n",
    "\n",
    "* The **Best-case complexity** of the algorithm is the function defined by the **minimum number of steps** taken in any instance of size _n_. This represents the curve passing through the lowest point of each column.\n",
    "\n",
    "    <br/>\n",
    "\n",
    "    In case of the search algorithm, the best case is when the `query` is the first element in `data`, and the algorithm only has to go through one element in the list to determine that. \n",
    "\n",
    "    In other words, in the best case, the amount of time it takes to run the algorithm is independent of the size of the input.\n",
    "\n",
    "    The best case is rarely used in practice, because it is not a good indicator of how the algorithm will perform in general. In fact, probabilistically speaking, the best case is the least likely to happen.\n",
    "\n",
    "    <br/>\n",
    "* The **Average-case complexity** of the algorithm, which is the function defined by the **average number of steps** over all instances of size _n_.\n",
    "\n",
    "    <br/>\n",
    "\n",
    "    In case of the search algorithm, the average case is when the `query` is in the middle of `data`, and the algorithm has to go through half the elements in the list to determine that. This is assuming that the `query` is equally likely to be in any position in the list i.e. incoming queries are uniformly distributed.\n",
    "\n",
    "The important thing to realize is that each of these time complexities define a numerical function, representing time versus problem size. These functions are as well defined as any other numerical function, be it $y = x^2 − 2x + 1$ or the price of Google stock as a function of time. But time complexities are such complicated functions that we must simplify them to work with them. For this, we need the “Big Oh” notation and the concept of asymptotic behavior.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $O$, $\\Omega$, and $\\Theta$ \n",
    "Assessing algorithmic performance makes use of the “big Oh” notation that, proves essential to compare algorithms and design more efficient ones. \n",
    "\n",
    "\n",
    "While the hopelessly _\"practical\"_ person may blanch at the notion of theoretical analysis, we present the material because it really is useful in thinking about algorithms.\n",
    "\n",
    "This method of keeping score will be the most mathematically demanding part of this book. But once you understand the intuition behind these ideas, the formalism becomes a lot easier to deal with.\n",
    "\n",
    "\n",
    "```{figure} ../assets/bigoh2.png\n",
    "---\n",
    "width: 60%\n",
    "name: fig-2-2\n",
    "---\n",
    "Upper and lower bounds valid for $n > n_0$ smooth out the bumps of complex functions. \n",
    "```\n",
    "\n",
    "The best, worst, and average-case time complexities for any given algorithm are numerical functions over the size of possible problem instances. However, it is very difficult to work precisely with these functions, because they tend to:\n",
    "\n",
    "* _Have too many bumps_ – The _exact_ time complexity function for any algorithm is liable to be very complicated with many little up and down bumps as shown in black curve in Figure 7.7. below. This is because the exact number of steps taken by an algorithm depends upon the exact input instance and the space of input instances is wide and varied. \n",
    "\n",
    "* _Require too much detail to specify precisely_ – Counting the exact number of RAM instructions executed in the worst case requires the algorithm be specified to the detail of a complete computer program. Further, the precise answer depends upon uninteresting coding details (e.g., did he use a case statement or nested ifs?). Performing a precise worst-case analysis like\n",
    "\n",
    "$$T(n)=12754n^2 +4353n+834lg_2n+13546$$\n",
    "\n",
    "would clearly be very difficult work, but provides us little extra information than the observation that “the time grows quadratically with n.”\n",
    "\n",
    "It proves to be much easier to talk in terms of simple upper and lower bounds of time-complexity functions using the Big Oh notation. The Big Oh simplifies our analysis by ignoring levels of detail that do not impact our comparison of algorithms.\n",
    "\n",
    "The Big Oh notation ignores the difference between multiplicative constants. The functions $f(n) = 2n$ and $g(n) = n$ are identical in Big Oh analysis. This makes sense given our application. Suppose a given algorithm in (say) C language ran twice as fast as one with the same algorithm written in Java. This multiplicative factor of two tells us nothing about the algorithm itself, since both programs implement exactly the same algorithm. We ignore such constant factors when comparing two algorithms.\n",
    "\n",
    "\n",
    "\n",
    "```{figure} ../assets/bigoh3.png\n",
    "---\n",
    "width: 100%\n",
    "name: fig-2-2\n",
    "---\n",
    "Illustrating the big $O$, big $\\Omega$, and big $\\Theta$ notations.\n",
    "```\n",
    "\n",
    "The formal definitions associated with the Big Oh notation are as follows:\n",
    "\n",
    "* **Big Oh $~O(\\cdot)$** \n",
    "    \n",
    "    <br/>\n",
    "\n",
    "    $f (n) = O(g(n))$ means $c · g(n)$ is an _upper bound_ on $f(n)$. \n",
    "\n",
    "    Thus there exists some constant c such that f (n) is always $\\leq c · g(n)$, for large enough $n$ (i.e. , $n \\geq n_0$ for some constant $n_0$).\n",
    "\n",
    "    $O$ is the function that gives the **upper bound** on the running time of an algorithm. In other words, it gives the **worst case** running time of an algorithm.\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **Big Omega $~\\Omega(\\cdot)$** \n",
    "    \n",
    "    <br/>\n",
    "\n",
    "    $f(n) = \\Omega(g(n))$ means $c · g(n)$ is a _lower bound_ on f(n). \n",
    "\n",
    "    Thus there exists some constant $c$ such that $f(n)$ is always $\\geq c · g(n)$, for all $n \\geq n_0$.\n",
    "\n",
    "    $\\Omega$ is the function that gives the **lower bound** on the running time of an algorithm. In other words, it gives the **best case** running time of an algorithm.\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **Big Theta $~\\Theta(\\cdot)$** \n",
    "    \n",
    "    <br/>\n",
    "\n",
    "    $f(n) = \\Theta(g(n))$ means $c_1 · g(n)$ is an _upper bound_ on f(n) and $c_2 · g(n)$ is a lower bound on $f(n)$, for all $n \\geq n_0$. \n",
    "\n",
    "    Thus there exist constants $c_1$ and $c_2$ such that $f(n) ≤ c_1 · g(n)$ and $f(n) ≥ $c_2$ · g(n)$. This means that $g(n)$ provides a nice, tight bound on f(n).\n",
    "\n",
    "    $\\Theta$ is the function that gives the **tight bound** on the running time of an algorithm. In other words, it gives the **average case** running time of an algorithm.\n",
    "\n",
    "<br/>\n",
    "\n",
    "These definitions are illustrated in the figure above. Each of these definitions assumes a constant $n_0$ beyond which they are always satisfied. We are not concerned about small values of $n$ (i.e. , anything to the left of $n_0$). After all, we don’t really care whether one sorting algorithm sorts six items faster than another, but seek which algorithm proves faster when sorting 10,000 or 1,000,000 items. The Big Oh notation enables us to ignore details and focus on the big picture.\n",
    "\n",
    "Make sure you understand this notation by working through the following examples. We choose certain constants ($c$ and $n_0$) in the explanations below because they work and make a point, but other pairs of constants will do exactly the same job. You are free to choose any constants that maintain the same inequality—ideally constants that make it obvious that the inequality holds:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Time Complexities\n",
    "\n",
    "\n",
    "The good news is that only a few function classes tend to occur in the course of basic algorithm analysis. These suffice to cover almost all the algorithms we will discuss in this text, and are listed in order of increasing dominance:\n",
    "\n",
    "\n",
    "```{figure} ../assets/bigoh4.png\n",
    "---\n",
    "width: 100%\n",
    "name: fig-2-4\n",
    "---\n",
    "Growth rates of common time complexities.\n",
    "```\n",
    "\n",
    "\n",
    "* **Constant functions**, $f(n) = 1$\n",
    "\n",
    "    Such functions might measure the cost of adding two numbers, printing out “The Star Spangled Banner,” or the growth realized by functions such as f (n) = min(n, 100). In the big picture, there is no dependence on the parameter n.\n",
    "\n",
    "* **Logarithmic** functions, $f (n) = log n$\n",
    "\n",
    "    Logarithmic time-complexity shows up in algorithms such as binary search. Such functions grow quite slowly as n gets big, but faster than the constant function (which is standing still, after all). \n",
    "\n",
    "    If you are unfamiliar with logarithms, please review [this page](https://fahadsultan.com/datascience_ml/2_maths/13_log.html) or come to office hours.\n",
    "\n",
    "* **Linear** functions, $f(n) = n$\n",
    "\n",
    "    Such functions measure the cost of looking at each item once (or twice, or ten times) in an n-element array, say to identify the biggest item, the smallest item, or compute the average value. If your code has a single loop over the input, it is probably linear. Even if your code has more than one loop, it may still be linear as long as no loop is nested inside another.\n",
    "\n",
    "* **Superlinear** functions, $f(n) = n lg n$\n",
    "\n",
    "    This important class of functions arises in such common in-place sorting algorithms. They grow just a little faster than linear, just enough to be a different dominance class. Time complexities of the form $n lg n$ often result from algorithms that divide problems in half at each step, and are called divide-and-conquer algorithms.\n",
    "\n",
    "* **Quadratic** functions, $f(n) = n^2$\n",
    "\n",
    "    Such functions measure the cost of looking at most or all pairs of items in an n-element universe. This arises in slow sorting algorithms. If your code has two nested loops over the input, it is probably quadratic.\n",
    "\n",
    "* **Cubic** functions, $f(n) = n^3$\n",
    "\n",
    "    Such functions enumerate through all triples of items in an $n$-element universe. \n",
    "    \n",
    "    These also arise in certain dynamic programming algorithms developed. If your code has three nested loops over the input, it is probably cubic.\n",
    "\n",
    "* **Polynomial** functions, $f(n) = n^k$ for a given constant $k > 1$ \n",
    "\n",
    "    Note that linear, quadratic and cubic functions are all special cases of polynomial functions. Polynomial functions are relevant in Computational Theory and the long unsolved [P vs NP problem](https://en.wikipedia.org/wiki/P_versus_NP_problem) question.\n",
    "\n",
    "    Polynomial functions arise in algorithms that make multiple passes over the input, such as matrix multiplication. If your code has $k$ nested loops over the input, it is probably polynomial.\n",
    "\n",
    "* **Exponential** functions, $f(n) = c^n$ for a given constant $c > 1$\n",
    "\n",
    "    Functions like $2^n$ arise when enumerating all subsets of n items. Exponential algorithms are uselessly fast. \n",
    "\n",
    "    If your code has $n$ nested loops over the input, it is probably exponential.\n",
    "\n",
    "* **Factorial** functions, $f(n) = n!$\n",
    "\n",
    "    Functions like $n!$ arise when generating all permutations or orderings of $n$ items.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "The table below lists the amount of time in seconds for each of the most common time complexities for a given input size $n$. \n",
    "\n",
    "```{figure} ../assets/skiena_bigo.png\n",
    "---\n",
    "width: 100%\n",
    "name: fig-2-4\n",
    "---\n",
    "Growth rates of common functions measured in seconds.\n",
    "```\n",
    "\n",
    "Note that if it takes **1 second** for an **O(n) algorithm** to process 1 billion elements (last row), it takes **31.7 years** for an **O($n^2$) algorithm** to process the same number of elements.\n",
    "\n",
    "In other words, algorithms with a time complexity worse than $O(n logn)$ are not at all scalable for any modern day data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
