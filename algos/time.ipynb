{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Complexity\n",
    "\n",
    "Consider the block of code that follows and answer the Search questions below.\n",
    "\n",
    "**Question:** Given **two inputs**: 1. list of n numbers `data` and 2. a number called `query`, return **output** last location of query in list, if it exists, otherwise return -1\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def search(data, query):         # 1\n",
    "   for i in range(len(data)):    # 2\n",
    "      if data[i] == query:       # 3\n",
    "         return i                # 4\n",
    "   return -1                     # 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function `search` above evaluate how many lines of code are executed for the following inputs: \n",
    "\n",
    "1. `data=range(10)` and `query = 0`\n",
    "2. `data=range(10)` and `query = 5`\n",
    "3. `data=range(10)` and `query = 10`\n",
    "4. `data=range(1000*1000)` and `query = 0`\n",
    "5. `data=range(1000*1000)` and `query = 1000*1000`\n",
    "\n",
    "* What is the best case scenario (fewest lines of code executed)? \n",
    "* What is the worst case scenario (most lines of code executed)? \n",
    "* What is the average case scenario (average number of lines of code executed)?\n",
    "\n",
    "```{figure} ../assets/trace_table.png\n",
    "---\n",
    "width: 100%\n",
    "name: trace_table\n",
    "---\n",
    "Use the table above to trace the search algorithm/code above. Add rows to the table as needed.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best, Worst, and Average-Case Complexity\n",
    "\n",
    "Using the RAM model of computation, we can count how many steps our algorithm takes on any given input instance by executing it. \n",
    "\n",
    "However, to understand **how good or bad** an algorithm is in general, we must know how it works **over ALL instances** (inputs).\n",
    "\n",
    "To understand the notions of the best, worst, and average-case complexity, think about running an algorithm over all possible instances of data that can be fed to it. For the problem of sorting, the set of possible input instances consists of all possible arrangements of n elements, over all possible values of n. \n",
    "\n",
    "We can represent each input instance as a point on a graph where the x-axis represents the size of the input problem (size of data), and the y-axis denotes the number of steps taken by the algorithm in this instance.\n",
    "\n",
    "```{figure} https://miro.medium.com/v2/resize:fit:4800/format:webp/1*1WlWN-d_vqf-_o48bWMjNA.png\n",
    "---\n",
    "height: 300px\n",
    "name: fig-2-1\n",
    "---\n",
    "The best, worst, and average-case complexities of an algorithm.\n",
    "```\n",
    "\n",
    "These points naturally align themselves into columns, because only integers represent possible input size (e.g., it makes no sense to sort 10.57 items). We can define three interesting functions over the plot of these points:\n",
    "\n",
    "* The **worst-case complexity** of the algorithm is the function defined by the **maximum number of steps** taken in any instance of size _n_. This represents the curve passing through the highest point in each column.\n",
    "\n",
    "* The **best-case complexity** of the algorithm is the function defined by the **minimum number of steps** taken in any instance of size _n_. This represents the curve passing through the lowest point of each column.\n",
    "\n",
    "* The **average-case complexity** of the algorithm, which is the function defined by the **average number of steps** over all instances of size _n_.\n",
    "\n",
    "The **worst-case complexity** proves to be **most useful** of these three measures **in practice**. Many people find this counterintuitive. \n",
    "\n",
    "To illustrate why, try to project what will happen if you bring n dollars into a casino to gamble. The best case, that you walk out owning the place, is possible but so unlikely that you should not even think about it. The worst case, that you lose all n dollars, is easy to calculate and distressingly likely to happen. The average case, that the typical bettor loses 87.32% of the money that he brings to the casino, is difficult to establish and its meaning subject to debate. What exactly does average mean? Stupid people lose more than smart people, so are you smarter or stupider than the average person, and by how much? Card counters at blackjack do better on average than customers who accept three or more free drinks. We avoid all these complexities and obtain a very useful result by just considering the worst case.\n",
    "\n",
    "The important thing to realize is that each of these time complexities define a numerical function, representing time versus problem size. These functions are as well defined as any other numerical function, be it $y = x^2 − 2x + 1$ or the price of Google stock as a function of time. But time complexities are such complicated functions that we must simplify them to work with them. For this, we need the “Big Oh” notation and the concept of asymptotic behavior.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Big Oh Notation\n",
    "\n",
    "\n",
    "Assessing algorithmic performance makes use of the “big Oh” notation that, proves essential to compare algorithms and design more efficient ones. \n",
    "\n",
    "\n",
    "While the hopelessly _\"practical\"_ person may blanch at the notion of theoretical analysis, we present the material because it really is useful in thinking about algorithms.\n",
    "\n",
    "\n",
    "This method of keeping score will be the most mathematically demanding part of this book. But once you understand the intuition behind these ideas, the formalism becomes a lot easier to deal with.\n",
    "\n",
    "The best, worst, and average-case time complexities for any given algorithm are numerical functions over the size of possible problem instances. However, it is very difficult to work precisely with these functions, because they tend to:\n",
    "\n",
    "* _Have too many bumps_ – An algorithm such as binary search typically runs a bit faster for arrays of size exactly n = 2k − 1 (where k is an integer), because the array partitions work out nicely. This detail is not particularly significant, but it warns us that the exact time complexity function for any algorithm is liable to be very complicated, with little up and down bumps as shown in Figure 2.2.\n",
    "\n",
    "* _Require too much detail to specify precisely_ – Counting the exact number of RAM instructions executed in the worst case requires the algorithm be specified to the detail of a complete computer program. Further, the precise answer depends upon uninteresting coding details (e.g., did he use a case statement or nested ifs?). Performing a precise worst-case analysis like\n",
    "\n",
    "$$T(n)=12754n^2 +4353n+834lg_2n+13546$$\n",
    "\n",
    "would clearly be very difficult work, but provides us little extra information than the observation that “the time grows quadratically with n.”\n",
    "\n",
    "It proves to be much easier to talk in terms of simple upper and lower bounds of time-complexity functions using the Big Oh notation. The Big Oh simplifies our analysis by ignoring levels of detail that do not impact our comparison of algorithms.\n",
    "\n",
    "The Big Oh notation ignores the difference between multiplicative constants. The functions f(n) = 2n and g(n) = n are identical in Big Oh analysis. This makes sense given our application. Suppose a given algorithm in (say) C language ran twice as fast as one with the same algorithm written in Java. This multiplicative factor of two tells us nothing about the algorithm itself, since both programs imple- ment exactly the same algorithm. We ignore such constant factors when comparing two algorithms.\n",
    "\n",
    "```{figure} ../assets/bigoh2.png\n",
    "---\n",
    "width: 60%\n",
    "name: fig-2-2\n",
    "---\n",
    "Upper and lower bounds valid for $n > n_0$ smooth out the bumps of complex functions. \n",
    "```\n",
    "\n",
    "\n",
    "```{figure} ../assets/bigoh3.png\n",
    "---\n",
    "width: 100%\n",
    "name: fig-2-2\n",
    "---\n",
    "Illustrating the big $O$, big $\\Omega$, and big $\\Theta$ notations.\n",
    "```\n",
    "\n",
    "The formal definitions associated with the Big Oh notation are as follows:\n",
    "\n",
    "* $f (n) = O(g(n))$ means $c · g(n)$ is an _upper bound_ on f (n). Thus there exists some constant c such that f (n) is always $\\leq c · g(n)$, for large enough n (i.e. , n ≥ n0 for some constant n0).\n",
    "\n",
    "* $f(n) = \\Omega(g(n))$ means $c · g(n)$ is a _lower bound_ on f(n). Thus there exists some constant c such that f(n) is always $\\geq c · g(n)$, for all n ≥ n0.\n",
    "\n",
    "* $f(n) = \\Theta(g(n))$ means $c_1 · g(n)$ is an _upper bound_ on f(n) and $c_2 · g(n)$ is a lower bound on $f(n)$, for all $n \\geq n0$. Thus there exist constants c1 and c2 such that $f(n) ≤ c1 · g(n)$ and $f(n) ≥ c2 · g(n)$. This means that $g(n)$ provides a nice, tight bound on f(n).\n",
    "\n",
    "Got it? These definitions are illustrated in the figure above. Each of these definitions assumes a constant n0 beyond which they are always satisfied. We are not concerned about small values of n (i.e. , anything to the left of n0). After all, we don’t really care whether one sorting algorithm sorts six items faster than another, but seek which algorithm proves faster when sorting 10,000 or 1,000,000 items. The Big Oh notation enables us to ignore details and focus on the big picture.\n",
    "\n",
    "Make sure you understand this notation by working through the following examples. We choose certain constants (c and n0) in the explanations below because they work and make a point, but other pairs of constants will do exactly the same job. You are free to choose any constants that maintain the same inequality—ideally constants that make it obvious that the inequality holds:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that only a few function classes tend to occur in the course of basic algorithm analysis. These suffice to cover almost all the algorithms we will discuss in this text, and are listed in order of increasing dominance:\n",
    "\n",
    "* **Constant functions**, $f(n) = 1$\n",
    "\n",
    "Such functions might measure the cost of adding two numbers, printing out “The Star Spangled Banner,” or the growth realized by functions such as f (n) = min(n, 100). In the big picture, there is no dependence on the parameter n.\n",
    "\n",
    "* **Logarithmic** functions, $f (n) = log n$\n",
    "\n",
    "Logarithmic time-complexity shows up in algorithms such as binary search. Such functions grow quite slowly as n gets big, but faster than the constant function (which is standing still, after all). \n",
    "\n",
    "If you are unfamiliar with logs, please review [this page](https://fahadsultan.com/datascience_ml/2_maths/13_log.html) or come to office hours.\n",
    "\n",
    "* **Linear** functions, $f(n) = n$\n",
    "\n",
    "Such functions measure the cost of looking at each item once (or twice, or ten times) in an n-element array, say to identify the biggest item, the smallest item, or compute the average value.\n",
    "\n",
    "* **Superlinear** functions, $f(n) = n lg n$\n",
    "\n",
    "This important class of functions arises in such algorithms as Quicksort and Mergesort. They grow just a little faster than linear (see Figure 2.4), just enough to be a different dominance class.\n",
    "\n",
    "* **Quadratic** functions, $f(n) = n^2$\n",
    "\n",
    "Such functions measure the cost of looking at most or all pairs of items in an n-element universe. This arises in algorithms such as insertion sort and selection sort.\n",
    "\n",
    "* **Cubic** functions, $f(n) = n^3$\n",
    "\n",
    "Such functions enumerate through all triples of items in an n-element universe. These also arise in certain dynamic program- ming algorithms developed in Chapter 8.\n",
    "\n",
    "* **Exponential** functions, $f(n) = c^n$ for a given constant $c > 1$\n",
    "\n",
    "Functions like $2^n$ arise when enumerating all subsets of n items. As we have seen, exponential algorithms become useless fast, but not as fast as. . .\n",
    "\n",
    "* **Factorial** functions, $f(n) = n!$\n",
    "\n",
    "Functions like n! arise when generating all permutations or orderings of n items."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../assets/skiena_bigo.png\n",
    "---\n",
    "width: 100%\n",
    "name: fig-2-4\n",
    "---\n",
    "Growth rates of common functions measured in nanoseconds.\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../assets/bigoh4.png\n",
    "---\n",
    "width: 100%\n",
    "name: fig-2-4\n",
    "---\n",
    "Growth rates of common growth functions.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
